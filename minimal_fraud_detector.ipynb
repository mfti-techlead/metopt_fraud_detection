{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, atan2, radians, log, log1p\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "import copy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from category_encoders import TargetEncoder # bonus: sklearn-contrib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_url(url, file_name):\n",
    "    \"\"\"Обучающая и тестовая выборки предварительно загружены на яндекс-диск\n",
    "    Функция выполняет скачивание и загрузку в датафрейм csv-файла по заданной ссылке\"\"\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(file_name, 'wb').write(r.content)\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_csv_from_url('https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/BXKOOeLfLQvfqw', 'fraudTrain.csv')\n",
    "test  = read_csv_from_url('https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/uL7gLcpQkJLUsw', 'fraudTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>28654</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>99160</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>83252</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>MT</td>\n",
       "      <td>59632</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>VA</td>\n",
       "      <td>24433</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num                            merchant       category     amt      first     last gender                        street            city state    zip      lat      long  city_pop                                job         dob                         trans_num   unix_time  merch_lat  merch_long  is_fraud\n",
       "0           0   2019-01-01 00:00:18  2703186189652095          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer    Banks      F                561 Perry Cove  Moravian Falls    NC  28654  36.0788  -81.1781      3495          Psychologist, counselling  1988-03-09  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315         0\n",
       "1           1   2019-01-01 00:00:44      630423337322     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie     Gill      F  43039 Riley Greens Suite 393          Orient    WA  99160  48.8878 -118.2105       149  Special educational needs teacher  1978-06-21  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462         0\n",
       "2           2   2019-01-01 00:00:51    38859492057661                fraud_Lind-Buckridge  entertainment  220.11     Edward  Sanchez      M      594 White Dale Suite 530      Malad City    ID  83252  42.1808 -112.2620      4154        Nature conservation officer  1962-01-19  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481         0\n",
       "3           3   2019-01-01 00:01:16  3534093764340240  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy    White      M   9443 Cynthia Court Apt. 038         Boulder    MT  59632  46.2306 -112.1138      1939                    Patent attorney  1967-01-12  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071         0\n",
       "4           4   2019-01-01 00:03:06   375534208663984                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   Garcia      M              408 Bradley Rest        Doe Hill    VA  24433  38.4207  -79.4629        99     Dance movement psychotherapist  1986-03-28  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459         0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(lat1, long1, lat2, long2):\n",
    "    \"\"\"Определение дистанции между точками\"\"\"\n",
    "    R = 6373.0 # примерный радиус Земли в км\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(long1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(long2)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_basic(df):\n",
    "    \"\"\"Функция добавляет к датафрейму аттрибуты:\n",
    "    - trans_hour     - час транзакции;\n",
    "    - trans_week_day - день недели транзакции;\n",
    "    - birth_year     - год рождения плательщика;\n",
    "    - trans_distance - дистанция между местом жительства плательщика и местом транзакции;\n",
    "    - trans_hex_id   - пространственный индекс места совершения операции (см. https://eng.uber.com/h3/)\n",
    "    \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: trans_hour, trans_dttm, dob, trans_week_day, trans_hex_id')\n",
    "    df['trans_hour'] = df.loc[:,'trans_date_trans_time'].str[11:13]\n",
    "    df['trans_dttm'] = pd.to_datetime(df.loc[:,'trans_date_trans_time'])\n",
    "    df['birth_year'] = df.loc[:,'dob'].str[0:4].astype('int')\n",
    "    df['trans_week_day'] = df.loc[:,'trans_dttm'].apply(lambda x: x.strftime('%A'))\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: trans_distance')\n",
    "    df['trans_distance'] = df.apply(lambda row: calc_distance(row[\"lat\"], row[\"long\"], row[\"merch_lat\"], row[\"merch_long\"]), axis = 1)\n",
    "    #df['trans_hex_id'] = df.apply(lambda row: h3.geo_to_h3(row[\"merch_lat\"], row[\"merch_long\"], resolution = 3), axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_share_by_feature(feature_name, target_name, df):\n",
    "    \"\"\"Функция рассчитывает характеристику встречаемости фрода для каждого значения\n",
    "    категориального аттрибута.\n",
    "    Возвращает датафрейм из 3х колонок: наименование аттрибута, количество единичек, долю единичек.\n",
    "    \"\"\"\n",
    "    agg_df = df.groupby([feature_name,target_name], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    pivot_df = agg_df.pivot(index=feature_name, columns=target_name).fillna(0)\n",
    "    flat_df = pd.DataFrame(pivot_df.to_records())\n",
    "    flat_df.columns = [feature_name, 'target_0', 'target_1']\n",
    "    flat_df['target_share'] = flat_df['target_1'] / (flat_df['target_0'] + flat_df['target_1'])\n",
    "    flat_df.sort_values(by='target_share', ascending=False, inplace = True)\n",
    "    flat_df.columns = [feature_name, feature_name + '_target_0_cnt', feature_name + '_target_1_cnt', feature_name + '_target_share']\n",
    "    flat_df = flat_df.drop(feature_name + '_target_0_cnt', axis = 1)\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_typical_zip_ref(train, min_typical_trans_cnt = 400):\n",
    "    \"\"\"Создадим справочник типовых локаций\"\"\"\n",
    "    zip_df = train.loc[train['is_fraud']==0,:].groupby(['zip','is_fraud'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    typical_zip = list(zip_df.loc[zip_df['cnt']>min_typical_trans_cnt,:]['zip'])\n",
    "    return typical_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_typical_job_ref(train, min_typical_trans_cnt = 400):\n",
    "    \"\"\"Создадим справочник типовых должностей\"\"\"\n",
    "    job_df = train.loc[train['is_fraud']==0,:].groupby(['job','is_fraud'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    typical_job = list(job_df.loc[job_df['cnt']>min_typical_trans_cnt,:]['job'])\n",
    "    return typical_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_typical_features(train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Добавим фичи:\n",
    "    - typical_zip - признак типовой локации\n",
    "    - typical_job - признак типовой должности\n",
    "    \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: mark typical_zip, typical_job')\n",
    "    typical_zip_ref = get_typical_zip_ref(train)\n",
    "    typical_job_ref = get_typical_job_ref(train)\n",
    "    if type_cd == 'train':\n",
    "        df = train\n",
    "    if type_cd == 'test':\n",
    "        df = test\n",
    "    df['typical_zip'] = df['zip'].isin(typical_zip_ref).astype(int)\n",
    "    df['typical_job'] = df['job'].isin(typical_job_ref).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_earlier_fraudsters(train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Разметим карточки, которые ранее были уличены во фроде\n",
    "    Функция добавляет к датафрейму колонку earlier_fraudster со значениями 1 или 0\n",
    "    В train обязательно устраняем лик, когда разметка берется из будущего\"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: mark earlier fraudsters')\n",
    "    fraudster_list = list(set(train.loc[train['is_fraud']==1,'cc_num']))\n",
    "    if type_cd == 'train':\n",
    "        first_fraud_dttm = train.loc[train['is_fraud']==1,:]\\\n",
    "        .groupby(['cc_num','is_fraud'], as_index=False)\\\n",
    "        .aggregate({'unix_time' : 'min'})\n",
    "        first_fraud_dttm['first_fraud_flag']=1\n",
    "        train = train.merge(first_fraud_dttm, on=['cc_num','unix_time'], how='left')\\\n",
    "                     .rename(columns = {'is_fraud_x':'is_fraud'})\n",
    "        train['earlier_fraudster'] = train['cc_num'].isin(fraudster_list).astype(int)\n",
    "        train.loc[train['first_fraud_flag']==1,'earlier_fraudster'] = 0\n",
    "        train.drop(['is_fraud_y','first_fraud_flag'], axis = 1, inplace = True)\n",
    "        return train\n",
    "    if type_cd == 'test':\n",
    "        test['earlier_fraudster'] = test['cc_num'].isin(fraudster_list).astype(int)\n",
    "        return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_transaction_time(train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Определим время первой транзакции для каждой карточки\"\"\"\n",
    "    if type_cd == 'train':\n",
    "        cc_num_min_time = train.groupby(['cc_num'], as_index=False)\\\n",
    "                .aggregate({'unix_time' : 'min'})\\\n",
    "                .rename(columns = {'unix_time': 'first_trans_time'})\n",
    "        return cc_num_min_time\n",
    "    if type_cd == 'test':\n",
    "        frames = [train, test]\n",
    "        train_test_concat = pd.concat(frames)\n",
    "        cc_num_min_time = train_test_concat.groupby(['cc_num'], as_index=False)\\\n",
    "                .aggregate({'unix_time' : 'min'})\\\n",
    "                .rename(columns = {'unix_time': 'first_trans_time'})\n",
    "        return cc_num_min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_life_time_days(train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Посчитаем время жизни карточки с момента первой транзакции\"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: calc life time days')\n",
    "    cc_num_min_time = get_first_transaction_time(train, test, type_cd)\n",
    "    if type_cd == 'train':\n",
    "        df = train\n",
    "    if type_cd == 'test':\n",
    "        df = test\n",
    "    df = df.merge(cc_num_min_time, on='cc_num', how='left')\n",
    "    df['life_time_days'] = df['unix_time'] - df['first_trans_time']\n",
    "    df['life_time_days'].fillna(0, inplace = True)\n",
    "    df['life_time_days'] = df['life_time_days'] / (60*60*24)\n",
    "    df['life_time_days'] = df['life_time_days'].apply(lambda x: x + random.uniform(0.05,0.5))\n",
    "    df['life_time_days'] = df['life_time_days'].round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_merchant_fraud_share(train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Определим долю фрода по каждому продавцу\"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: mark merchant fraud share')\n",
    "    merchant_fraud_share = target_share_by_feature('merchant', 'is_fraud', train)\n",
    "    merchant_fraud_share.drop('merchant_target_1_cnt', axis = 1, inplace = True)\n",
    "    if type_cd == 'train':\n",
    "        df = train\n",
    "    if type_cd == 'test':\n",
    "        df = test\n",
    "    df = df.merge(merchant_fraud_share, on='merchant', how='left')\n",
    "    df['merchant_target_share'].fillna(0, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unwanted_features(df):\n",
    "    \"\"\" Удалим лишние аттрибуты \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Deleting unwanted features')\n",
    "    features_to_delete = ['Unnamed: 0','trans_date_trans_time','cc_num','merchant',\n",
    "                          'first','last','street','city','state','zip','lat','long',\n",
    "                          'job','dob','trans_num','unix_time','merch_lat','merch_long',\n",
    "                          'trans_dttm','first_trans_time']\n",
    "    df.drop(features_to_delete, axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_scale(feature_name_list, train, test = None, type_cd = 'train'):\n",
    "    \"\"\"Функция выполняет Z-масштабирование аттрибутов: (x - mean) / standard_deviation\n",
    "    \"\"\"\n",
    "    # Standardscaler - есть стандатная функция.\n",
    "    for feature_name in list(feature_name_list):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Z scale: ' + feature_name)\n",
    "        mean_x = train[feature_name].mean()\n",
    "        std_x = train[feature_name].std()\n",
    "        if type_cd == 'train':\n",
    "            df = train\n",
    "        if type_cd == 'test':\n",
    "            df = test\n",
    "        df[feature_name+'_stdnorm'] = df[feature_name].apply(lambda x: (x - mean_x) / (std_x))\n",
    "        df = df.drop(feature_name, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, feature_name_list):\n",
    "    \"\"\" Функция выполняет min-max нормализацию аттрибутов: (x - min_x) / (max_x-min_x)\n",
    "    \"\"\"\n",
    "    # MinMaxScaler (стандартная функция sklearn)\n",
    "    for feature_name in list(feature_name_list):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Min_max_scale: ' + feature_name)\n",
    "        min_x = df[feature_name].min()\n",
    "        max_x = df[feature_name].max()\n",
    "        df[feature_name+'_norm'] = df[feature_name].apply(lambda x: (x - min_x) / (max_x-min_x))\n",
    "        df = df.drop(feature_name, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-26 23:46:47 Preprocess features: trans_hour, trans_dttm, dob, trans_week_day, trans_hex_id\n",
      "2020-11-26 23:47:08 Preprocess features: trans_distance\n",
      "2020-11-26 23:49:32 Preprocess features: calc life time days\n"
     ]
    }
   ],
   "source": [
    "train = preprocess_features_basic(train)\n",
    "train = mark_life_time_days(train)\n",
    "train_init = copy.deepcopy(train)\n",
    "test_init = copy.deepcopy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-26 23:49:42 Preprocess features: mark typical_zip, typical_job\n",
      "2020-11-26 23:49:44 Preprocess features: mark earlier fraudsters\n",
      "2020-11-26 23:49:54 Preprocess features: mark merchant fraud share\n",
      "2020-11-26 23:50:01 Deleting unwanted features\n"
     ]
    }
   ],
   "source": [
    "train = mark_typical_features(train)\n",
    "train = mark_earlier_fraudsters(train)\n",
    "train = mark_merchant_fraud_share(train)\n",
    "train = delete_unwanted_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-26 23:50:02 Z scale: amt\n",
      "2020-11-26 23:50:03 Z scale: city_pop\n",
      "2020-11-26 23:50:05 Z scale: trans_distance\n",
      "2020-11-26 23:50:06 Z scale: life_time_days\n",
      "2020-11-26 23:50:07 Z scale: birth_year\n"
     ]
    }
   ],
   "source": [
    "features_to_scale = ['amt','city_pop', 'trans_distance', 'life_time_days', 'birth_year']\n",
    "train = z_scale(features_to_scale, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-26 23:50:08 Preprocess features: trans_hour, trans_dttm, dob, trans_week_day, trans_hex_id\n",
      "2020-11-26 23:50:17 Preprocess features: trans_distance\n",
      "2020-11-26 23:51:19 Preprocess features: calc life time days\n",
      "2020-11-26 23:51:24 Preprocess features: mark typical_zip, typical_job\n",
      "2020-11-26 23:51:26 Preprocess features: mark earlier fraudsters\n",
      "2020-11-26 23:51:26 Preprocess features: mark merchant fraud share\n",
      "2020-11-26 23:51:27 Deleting unwanted features\n",
      "2020-11-26 23:51:28 Z scale: amt\n",
      "2020-11-26 23:51:28 Z scale: city_pop\n",
      "2020-11-26 23:51:28 Z scale: trans_distance\n",
      "2020-11-26 23:51:29 Z scale: life_time_days\n",
      "2020-11-26 23:51:29 Z scale: birth_year\n"
     ]
    }
   ],
   "source": [
    "test = preprocess_features_basic(test)\n",
    "test = mark_life_time_days(train = train_init, test = test, type_cd = 'test')\n",
    "test = mark_typical_features(train = train_init, test = test, type_cd = 'test')\n",
    "test = mark_earlier_fraudsters(train = train_init, test = test, type_cd = 'test')\n",
    "test = mark_merchant_fraud_share(train = train_init, test = test, type_cd = 'test')\n",
    "test = delete_unwanted_features(test)\n",
    "test = z_scale(features_to_scale, train = train_init, test = test, type_cd = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = TargetEncoder(cols=train.select_dtypes(include=[object]).columns)\n",
    "estimator = XGBClassifier(nthread=-1, n_estimators=1000, learning_rate=0.1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('my_encoder', target_encoder),\n",
    "    ('my_estimator', estimator)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((816904, 18), (90768, 18), (389003, 18))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(train, train['is_fraud'],\n",
    "                                         stratify=train['is_fraud'],\n",
    "                                         test_size=0.3,\n",
    "                                         random_state=43)\n",
    "\n",
    "# отделяем от данных для обучения небольшой сэмпл для early_stopping\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                 stratify=y,\n",
    "                                                 test_size=0.1,\n",
    "                                                 random_state=43)\n",
    "\n",
    "# итого\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['is_fraud'], axis=1)\n",
    "X_val = X_val.drop(['is_fraud'], axis=1)\n",
    "X_test = X_test.drop(['is_fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "initial_steps = []\n",
    "for i in pipeline.steps:\n",
    "    initial_steps.append(clone(i[1]))\n",
    "\n",
    "transformers_pipeline = make_pipeline(*initial_steps[:-1])\n",
    "transformers_pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_val_transformed = transformers_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.96039\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[10]\tvalidation_0-auc:0.96604\n",
      "Stopping. Best iteration:\n",
      "[11]\tvalidation_0-auc:0.96621\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('my_encoder',\n",
       "                 TargetEncoder(cols=Index(['category', 'gender', 'trans_hour', 'trans_week_day'], dtype='object'),\n",
       "                               drop_invariant=False, handle_missing='value',\n",
       "                               handle_unknown='value', min_samples_leaf=1,\n",
       "                               return_df=True, smoothing=1.0, verbose=0)),\n",
       "                ('my_estimator',\n",
       "                 XGBClassifier(base_score=0.5, booster=None,\n",
       "                               colsample_bylevel=1, colsample_...\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=6,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=1000,\n",
       "                               n_jobs=-1, nthread=-1, num_parallel_tree=1,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method=None,\n",
       "                               validate_parameters=False, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# передаем в fit параметры для early_stopping как: my_estimator__eval_set\n",
    "# учится очень быстро -  смысл в том, что берутся случайным образом перемешанные данные\n",
    "# высокий градиент функции потерь\n",
    "pipeline.fit(X_train, y_train,\n",
    "            my_estimator__eval_set=[(X_val_transformed, y_val)], \n",
    "            my_estimator__eval_metric='auc', \n",
    "            my_estimator__early_stopping_rounds=5, \n",
    "            my_estimator__verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673787545745959"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real = test.drop('is_fraud', axis = 1)\n",
    "y_test_real = test.loc[:,'is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960994441687899"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict_proba(X_test_real)\n",
    "roc_auc_score(y_test_real, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(y_test_real,list(y_pred[:,1])))\n",
    "compare = pd.DataFrame(data_tuples, columns=['y_true','y_probas'])\n",
    "compare['y_probas'] = compare['y_probas'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare.to_excel('compare_8.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>typical_zip</th>\n",
       "      <td>0.219558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt</th>\n",
       "      <td>0.201036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_hour</th>\n",
       "      <td>0.191122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0.151919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.074960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birth_year_stdnorm</th>\n",
       "      <td>0.061552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earlier_fraudster</th>\n",
       "      <td>0.044733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merchant_target_share</th>\n",
       "      <td>0.031121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_pop</th>\n",
       "      <td>0.015540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_week_day</th>\n",
       "      <td>0.004728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life_time_days</th>\n",
       "      <td>0.003261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_distance</th>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typical_job</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_stdnorm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_pop_stdnorm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       importance\n",
       "typical_zip              0.219558\n",
       "amt                      0.201036\n",
       "trans_hour               0.191122\n",
       "category                 0.151919\n",
       "gender                   0.074960\n",
       "birth_year_stdnorm       0.061552\n",
       "earlier_fraudster        0.044733\n",
       "merchant_target_share    0.031121\n",
       "city_pop                 0.015540\n",
       "trans_week_day           0.004728\n",
       "life_time_days           0.003261\n",
       "trans_distance           0.000469\n",
       "typical_job              0.000000\n",
       "amt_stdnorm              0.000000\n",
       "city_pop_stdnorm         0.000000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi=pd.DataFrame({'importance':pipeline[1].feature_importances_},index=X_train.columns)\n",
    "fi.sort_values('importance',ascending=False).head(15)\n",
    "# берем фичу, по ней упорядочиваем весь датасет. Например отсортировали по возрасту. Смотрим как поверхность делит выборку и на какие состояния\n",
    "# например больши и меньше 30 лет. На этих областях считается empurity из пропорций\n",
    "# всего доля единичек - 1%. Меньше 30 - 0.5% а больше 30 - 0.5%\n",
    "# Критерий Джини: - 1 - вероятность 1 класса и 1 - вероятность 0 класса. \n",
    "# При каждом делении неопределенность должна уменьшаться. 1 минус сумма квадратов по одному классу и 1 минус сумма квадратов по другому классу. \n",
    "# Когда по какой-то фиче делим - получаем информационный прирост.\n",
    "# featureimportance - информационный прирост, полученный для каждой фичи. какая-то фича может фигурировать в нескольких уровнях дерева. \n",
    "# featureimportance - пример с шахматной доской (делим несколько раз вдоль и поперек)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier(nthread=-1, max_depth = 4, learning_rate=0.1, n_estimators=100, min_child_weight = 5, reg_alpha=0.01, reg_lambda = 0.1)\n",
    "# alpha gamma - регуляризация модели, чтобы избежать переобучения\n",
    "# для лог.регрессии - числа больше 0. Чем меньше число, тем больше регуляризация. Обычно от 0.0001 до 1000. \n",
    "# Регуляризация подстраивается под конкретные данные. \n",
    "# чем меньше значение, тем проще модель (в случае с логистической регрессией) чем больше лябда, тем больше регуляризация\n",
    "# Задача увеличить коэффициент регуляризации. \n",
    "# Бустинг: использует слабые эстиматоры (оценщики), которые идут друг за другом подряд. Получаем ошибку и даем на вход \n",
    "# Эстиматор - это дерево решений. (c набором параметров)\n",
    "# используется дерево решений с глубиной 6 по умолчанию \n",
    "# n_estimators - количество эстиматоров, идущих друг за другом\n",
    "# в каждом эстиматоре есть глубина - уменьшить глубину. max_depth - уменьшить (по умолчанию должно стоять 6)\n",
    "#reg_alpha - коэффициент регуляризации\n",
    "#reg_lambda - коэффициент регуляризации\n",
    "# learning_rate - характеризует скорость движения к минимуму функции потерь. Чем он меньше, тем медленнее и аккуратнее мы движемся.\n",
    "# при малых значениях скорее всего получим недообучение, т.к. наша функция потерь не будет меняться. \n",
    "# Идеальная статья про параметны xgboost http://biostat-r.blogspot.com/2016/08/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Варианты развития модели:\n",
    "# - Попробовать сбалансировать классы\n",
    "# - undersampling (проверить это в sklearn и imblearn)\n",
    "# - Попробовать oversampling\n",
    "# - Попробовать синтетическое добавление (imblearn - класс SMOTE)\n",
    "# - adasyn - adaptive syntetic generation - разбавить синтетическими данными, но мерить качество (валидироваться) без синтетических данных\n",
    "# - Для избежания переобучения - сократить количество фичей\n",
    "# - Регуляризация - добавление штрафа в функцию потерь. (L1 & L2 - регуляризация)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
