{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import shap\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from category_encoders import TargetEncoder # bonus: sklearn-contrib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('fraudTrain.csv')\n",
    "test  = pd.read_csv('fraudTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>28654</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>99160</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>83252</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>MT</td>\n",
       "      <td>59632</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>VA</td>\n",
       "      <td>24433</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num                            merchant       category     amt      first     last gender                        street            city state    zip      lat      long  city_pop                                job         dob                         trans_num   unix_time  merch_lat  merch_long  is_fraud\n",
       "0           0   2019-01-01 00:00:18  2703186189652095          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer    Banks      F                561 Perry Cove  Moravian Falls    NC  28654  36.0788  -81.1781      3495          Psychologist, counselling  1988-03-09  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315         0\n",
       "1           1   2019-01-01 00:00:44      630423337322     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie     Gill      F  43039 Riley Greens Suite 393          Orient    WA  99160  48.8878 -118.2105       149  Special educational needs teacher  1978-06-21  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462         0\n",
       "2           2   2019-01-01 00:00:51    38859492057661                fraud_Lind-Buckridge  entertainment  220.11     Edward  Sanchez      M      594 White Dale Suite 530      Malad City    ID  83252  42.1808 -112.2620      4154        Nature conservation officer  1962-01-19  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481         0\n",
       "3           3   2019-01-01 00:01:16  3534093764340240  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy    White      M   9443 Cynthia Court Apt. 038         Boulder    MT  59632  46.2306 -112.1138      1939                    Patent attorney  1967-01-12  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071         0\n",
       "4           4   2019-01-01 00:03:06   375534208663984                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   Garcia      M              408 Bradley Rest        Doe Hill    VA  24433  38.4207  -79.4629        99     Dance movement psychotherapist  1986-03-28  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(lat1, long1, lat2, long2):\n",
    "    \"\"\"Определение дистанции между точками\n",
    "    \"\"\"\n",
    "    R = 6373.0 # approximate radius of earth in km\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(long1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(long2)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_basic(df):\n",
    "    \"\"\"Функция добавляет к датафрейму аттрибуты:\n",
    "    - trans_hour     - час транзакции;\n",
    "    - trans_week_day - день недели транзакции;\n",
    "    - birth_year     - год рождения плательщика\n",
    "    - trans_distance - дистанция между местом жительства плательщика и местом транзакции\n",
    "    \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: trans_hour, trans_dttm, dob, trans_week_day')\n",
    "    df['trans_hour'] = df['trans_date_trans_time'].str[11:13]\n",
    "    df['trans_dttm'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['birth_year'] = df['dob'].str[0:4].astype('int')\n",
    "    df['trans_week_day'] = df['trans_dttm'].apply(lambda x: x.strftime('%A'))\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess features: trans_distance')\n",
    "    df['trans_distance'] = df.apply(lambda row: calc_distance(row[\"lat\"], row[\"long\"], row[\"merch_lat\"], row[\"merch_long\"]), axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_str(feature_name, target_name, df):\n",
    "    \"\"\"Определяем количество и долю единичек для аттрибута. \n",
    "    Функция возвращает датафрейм из 3х колонок: \n",
    "    наименование аттрибута, количество единичек, долю единичек для каждого значения аттрибута\n",
    "    \"\"\"\n",
    "    agg_df = df.groupby([feature_name,target_name], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    pivot_df = agg_df.pivot(index=feature_name, columns=target_name).fillna(0)\n",
    "    flat_df = pd.DataFrame(pivot_df.to_records())\n",
    "    flat_df.columns = [feature_name, 'target_0', 'target_1']\n",
    "    flat_df['target_share'] = flat_df['target_1'] / (flat_df['target_0'] + flat_df['target_1'])\n",
    "    flat_df.sort_values(by='target_share', ascending=False, inplace = True)\n",
    "    flat_df.columns = [feature_name, feature_name + '_target_0_cnt', feature_name + '_target_1_cnt', feature_name + '_target_share']\n",
    "    flat_df = flat_df.drop(feature_name + '_target_0_cnt', axis = 1)\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_references(train, test = None, type_cd = 'train', min_typical_trans_cnt = 400):\n",
    "    \"\"\"Функция добавляет к датафрейму аттбируты на основании справочников, полученных из обучающей выборки:\n",
    "    - life_time_days - количество дней между первой транзакцией плательщика и текущей (\"время жизни плательника\")\n",
    "    - typical_zip    - признак того, что код локации является типовым, т.е. в нем совершено большое количество (>400) нефродовых транзакций  \n",
    "    - typical_job    - признак того, что должность является типовой, т.е. для нее совершено большое количество (>400) нефродовых транзакций  \n",
    "    - merchant_target_share - доля фродовых транзакций у получателя платежа\n",
    "    \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Preprocess_features_references: zip, job, cc_num, merchant')\n",
    "    zip_df = train.loc[train['is_fraud']==0,:].groupby(['zip','is_fraud'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    typical_zip = list(zip_df.loc[zip_df['cnt']>min_typical_trans_cnt,:]['zip'])\n",
    "    job_df = train.loc[train['is_fraud']==0,:].groupby(['job','is_fraud'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})\n",
    "    typical_job = list(job_df.loc[job_df['cnt']>min_typical_trans_cnt,:]['job'])\n",
    "    fraudster = list(train.loc[train['is_fraud']==1,:].groupby(['cc_num','is_fraud'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'count'})\\\n",
    "            .rename(columns = {'unix_time': 'cnt'})['cc_num'])\n",
    "    cc_num_min_time = train.groupby(['cc_num'], as_index=False)\\\n",
    "            .aggregate({'unix_time' : 'min'})\\\n",
    "            .rename(columns = {'unix_time': 'first_trans_time'})\n",
    "    merchant_fraud_share = get_feature_str('merchant', 'is_fraud', train)\n",
    "    merchant_fraud_share.drop('merchant_target_1_cnt', axis = 1, inplace = True)\n",
    "    def mark_typical_features(df,typical_zip,typical_job):\n",
    "        df['typical_zip'] = df['zip'].isin(typical_zip).astype(int)\n",
    "        df['typical_job'] = df['job'].isin(typical_job).astype(int)\n",
    "        df['earlier_fraudster'] = df['cc_num'].isin(fraudster).astype(int)\n",
    "        df = df.merge(cc_num_min_time, on='cc_num', how='left')\n",
    "        df['life_time_days'] = df['unix_time'] - df['first_trans_time']\n",
    "        df['life_time_days'].fillna(0, inplace = True)\n",
    "        df['life_time_days'] = df['life_time_days'] / (60*60*24)\n",
    "        df['life_time_days'] = df['life_time_days'].apply(lambda x: x + random.uniform(0.05,0.5))\n",
    "        df['life_time_days'] = df['life_time_days'].round(3)\n",
    "        df = df.merge(merchant_fraud_share, on='merchant', how='left')\n",
    "        df['merchant_target_share'].fillna(0, inplace = True)\n",
    "        return df\n",
    "    if type_cd == 'train':\n",
    "        train = mark_typical_features(train,typical_zip,typical_job)\n",
    "        return train\n",
    "    if type_cd == 'test':\n",
    "        test = mark_typical_features(test,typical_zip,typical_job)\n",
    "        return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unwanted_features(df):\n",
    "    \"\"\" Удалим лишние аттрибуты\n",
    "    \"\"\"\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Deleting unwanted features')\n",
    "    features_to_delete = ['Unnamed: 0','trans_date_trans_time','cc_num','merchant',\n",
    "                          'first','last','street','city','state','zip','lat','long',\n",
    "                          'job','dob','trans_num','unix_time','merch_lat','merch_long',\n",
    "                          'trans_dttm','first_trans_time']\n",
    "    df.drop(features_to_delete, axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, col_name):\n",
    "    \"\"\" Функция выполняет one hot encoding по списку аттрибутов\n",
    "    \"\"\"\n",
    "    for i in list(col_name):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' One hot encoding: ' + i)\n",
    "        one_hot = pd.get_dummies(df[i])\n",
    "        one_hot.columns = i + '_' + one_hot.columns\n",
    "        df = df.drop(i, axis = 1)\n",
    "        df = df.join(one_hot)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(df, feature_name_list):\n",
    "    \"\"\" Функция выполняет min-max нормализацию аттрибутов: (x - min_x) / (max_x-min_x)\n",
    "    \"\"\"\n",
    "    for feature_name in list(feature_name_list):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Min_max_scale: ' + feature_name)\n",
    "        min_x = df[feature_name].min()\n",
    "        max_x = df[feature_name].max()\n",
    "        df[feature_name+'_norm'] = df[feature_name].apply(lambda x: (x - min_x) / (max_x-min_x))\n",
    "        df = df.drop(feature_name, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_scale(df, feature_name_list):\n",
    "    \"\"\"Функция выполняет Z-нормализацию аттрибутов: (x - mean) / standard_deviation\n",
    "    \"\"\"\n",
    "    for feature_name in list(feature_name_list):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) + ' Min_max_scale: ' + feature_name)\n",
    "        mean_x = df[feature_name].mean()\n",
    "        std_x = df[feature_name].std()\n",
    "        df[feature_name+'_stdnorm'] = df[feature_name].apply(lambda x: (x - mean_x) / (std_x))\n",
    "        df = df.drop(feature_name, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-10 22:54:55 Preprocess features: trans_hour, trans_dttm, dob, trans_week_day\n",
      "2020-11-10 22:55:08 Preprocess features: trans_distance\n",
      "2020-11-10 22:56:27 Preprocess_features_references: zip, job, cc_num, merchant\n",
      "2020-11-10 22:56:34 Deleting unwanted features\n"
     ]
    }
   ],
   "source": [
    "train_1 = preprocess_features_basic(train)\n",
    "train_2 = preprocess_features_references(train = train_1, type_cd = 'train')\n",
    "train_3 = delete_unwanted_features(train_2)\n",
    "#train_4 = one_hot_encode(train_3, ['category','gender','trans_hour', 'trans_week_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-10 22:56:35 Preprocess features: trans_hour, trans_dttm, dob, trans_week_day\n",
      "2020-11-10 22:56:40 Preprocess features: trans_distance\n",
      "2020-11-10 22:57:15 Preprocess_features_references: zip, job, cc_num, merchant\n",
      "2020-11-10 22:57:19 Deleting unwanted features\n"
     ]
    }
   ],
   "source": [
    "test_1 = preprocess_features_basic(test)\n",
    "test_2 = preprocess_features_references(train = train_1, test = test_1, type_cd = 'test')\n",
    "test_3 = delete_unwanted_features(test_2)\n",
    "#test_4 = one_hot_encode(test_3, ['category','gender','trans_hour', 'trans_week_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-10 23:06:56 Min_max_scale: amt\n",
      "2020-11-10 23:06:56 Min_max_scale: city_pop\n",
      "2020-11-10 23:06:57 Min_max_scale: trans_distance\n",
      "2020-11-10 23:06:58 Min_max_scale: life_time_days\n",
      "2020-11-10 23:06:58 Min_max_scale: merchant_target_share\n",
      "2020-11-10 23:06:59 Min_max_scale: birth_year\n"
     ]
    }
   ],
   "source": [
    "features_to_scale = ['amt','city_pop', 'trans_distance', 'life_time_days', \n",
    "                     'merchant_target_share','birth_year']\n",
    "train_5 = min_max_scale(train_3, features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = TargetEncoder(cols=train_5.select_dtypes(include=[object]).columns)\n",
    "estimator = XGBClassifier(nthread=-1, n_estimators=1000, learning_rate=0.1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('my_encoder', target_encoder),\n",
    "    ('my_estimator', estimator)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((816904, 14), (90768, 14), (389003, 14))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(train_5, train_5['is_fraud'],\n",
    "                                         stratify=train_5['is_fraud'],\n",
    "                                         test_size=0.3,\n",
    "                                         random_state=43)\n",
    "\n",
    "# отделяем от данных для обучения небольшой сэмпл для early_stopping\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                 stratify=y,\n",
    "                                                 test_size=0.1,\n",
    "                                                 random_state=43)\n",
    "\n",
    "# итого\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['is_fraud'], axis=1)\n",
    "X_val = X_val.drop(['is_fraud'], axis=1)\n",
    "X_test = X_test.drop(['is_fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "initial_steps = []\n",
    "for i in pipeline.steps:\n",
    "    initial_steps.append(clone(i[1]))\n",
    "\n",
    "transformers_pipeline = make_pipeline(*initial_steps[:-1])\n",
    "transformers_pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_val_transformed = transformers_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.96047\n",
      "Will train until validation_0-auc hasn't improved in 5 rounds.\n",
      "[10]\tvalidation_0-auc:0.96618\n",
      "Stopping. Best iteration:\n",
      "[12]\tvalidation_0-auc:0.96635\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('my_encoder',\n",
       "                 TargetEncoder(cols=Index(['category', 'gender', 'trans_hour', 'trans_week_day'], dtype='object'),\n",
       "                               drop_invariant=False, handle_missing='value',\n",
       "                               handle_unknown='value', min_samples_leaf=1,\n",
       "                               return_df=True, smoothing=1.0, verbose=0)),\n",
       "                ('my_estimator',\n",
       "                 XGBClassifier(base_score=0.5, booster=None,\n",
       "                               colsample_bylevel=1, colsample_...\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=6,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=1000,\n",
       "                               n_jobs=-1, nthread=-1, num_parallel_tree=1,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method=None,\n",
       "                               validate_parameters=False, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# передаем в fit параметры для early_stopping как: my_estimator__eval_set\n",
    "pipeline.fit(X_train, y_train,\n",
    "            my_estimator__eval_set=[(X_val_transformed, y_val)], \n",
    "            my_estimator__eval_metric='auc', \n",
    "            my_estimator__early_stopping_rounds=5, \n",
    "            my_estimator__verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678273182758805"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-10 23:07:41 Min_max_scale: amt\n",
      "2020-11-10 23:07:42 Min_max_scale: city_pop\n",
      "2020-11-10 23:07:42 Min_max_scale: trans_distance\n",
      "2020-11-10 23:07:42 Min_max_scale: life_time_days\n",
      "2020-11-10 23:07:43 Min_max_scale: merchant_target_share\n",
      "2020-11-10 23:07:43 Min_max_scale: birth_year\n"
     ]
    }
   ],
   "source": [
    "X_test_real = test_3.drop('is_fraud', axis = 1)\n",
    "X_test_real = min_max_scale(X_test_real, features_to_scale)\n",
    "y_test_real = test_3.loc[:,'is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9480965907801345"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipeline.predict_proba(X_test_real)\n",
    "roc_auc_score(y_test_real, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(y_test_real,list(y_pred[:,1])))\n",
    "compare = pd.DataFrame(data_tuples, columns=['y_true','y_probas'])\n",
    "compare['y_probas'] = compare['y_probas'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare.to_excel('compare_8.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trans_hour</th>\n",
       "      <td>0.198757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typical_zip</th>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_norm</th>\n",
       "      <td>0.189923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0.154310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earlier_fraudster</th>\n",
       "      <td>0.074473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.071177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birth_year_norm</th>\n",
       "      <td>0.066733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merchant_target_share_norm</th>\n",
       "      <td>0.031752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_pop_norm</th>\n",
       "      <td>0.012960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life_time_days_norm</th>\n",
       "      <td>0.006416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_distance_norm</th>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_week_day</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typical_job</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            importance\n",
       "trans_hour                    0.198757\n",
       "typical_zip                   0.193100\n",
       "amt_norm                      0.189923\n",
       "category                      0.154310\n",
       "earlier_fraudster             0.074473\n",
       "gender                        0.071177\n",
       "birth_year_norm               0.066733\n",
       "merchant_target_share_norm    0.031752\n",
       "city_pop_norm                 0.012960\n",
       "life_time_days_norm           0.006416\n",
       "trans_distance_norm           0.000399\n",
       "trans_week_day                0.000000\n",
       "typical_job                   0.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi=pd.DataFrame({'importance':pipeline[1].feature_importances_},index=X_train.columns)\n",
    "fi.sort_values('importance',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
